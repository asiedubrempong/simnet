{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network Module\n",
    "\n",
    "Implementation of SGD and backprop in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        sizes is a list of the number of neurons in each layer of the network\n",
    "        eg. [20, 10, 15] - 20 in the input layer \n",
    "                           10 in the hidden layer\n",
    "                           15 in the output layer\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        # randomly initialize the weights and biases of the network\n",
    "        self.weights = [np.random.randn(current, prev) \n",
    "                        for prev, current in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        self.biases = [np.random.randn(current, 1) \n",
    "                       for current in self.sizes[1:]]\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        returns the output if x is the input vector\n",
    "        \"\"\"\n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            z = np.dot(weight, x) + bias\n",
    "            x = self.sigmoid(z) # vector of activations for that layer\n",
    "        return x\n",
    "    \n",
    "    def SGD(self, training_data, epochs, bs, lr, test_data=None):\n",
    "        \"\"\"\n",
    "        epochs: number of epochs ie. number of times to go through the entire dataset\n",
    "        bs : batch size\n",
    "        lr: learining rate\n",
    "        \"\"\"\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            # randomly shuffle the data \n",
    "            random.shuffle(training_data)\n",
    "            # divide the data into mini-batches\n",
    "            mini_batches = [training_data[index:index+bs]\n",
    "                            for index in range(0, n, bs)]\n",
    "            for mini_batch in mini_batches:\n",
    "                # run one mini-batch\n",
    "                self.run_mini_batch(mini_batch, lr)\n",
    "            \n",
    "            if test_data:\n",
    "                test_data = list(test_data)\n",
    "                n_test = len(test_data)\n",
    "                # call evaluate to evaluate performance\n",
    "                number_correct = self.evaluate(test_data)\n",
    "                print(f'Epoch {i}: {number_correct} / {n_test}')\n",
    "            else:\n",
    "                print(f'Epoch {i} completed')\n",
    "    \n",
    "    def run_mini_batch(self, mini_batch, lr):\n",
    "        for x, y in mini_batch:\n",
    "            grad_biases, grad_weights = self.backprop(x, y)\n",
    "            \n",
    "            # update rule for stochastic gradient descent\n",
    "            self.weights = [old_weight - (lr/len(mini_batch))*gradient\n",
    "                            for old_weight, gradient in zip(self.weights, grad_weights)]\n",
    "            self.biases = [old_bias - (lr/len(mini_batch))*gradient\n",
    "                           for old_bias, gradient in zip(self.biases, grad_biases)]\n",
    "            \n",
    "    def backprop(self, x, y):\n",
    "        grad_biases = [np.zeros_like(bias) for bias in self.biases]\n",
    "        grad_weights = [np.zeros_like(weight) for weight in self.weights]\n",
    "        \n",
    "        # forward pass \n",
    "        activations = [x] # list of the activations at all layers in the network\n",
    "        zs = [] # list of the z vectors at all layers in the network\n",
    "        \n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            z = np.dot(weight, x) + bias\n",
    "            zs.append(z)\n",
    "            # cal the activation for that layer\n",
    "            x = self.sigmoid(z)\n",
    "            activations.append(x)\n",
    "            \n",
    "        # use the first equation to calculate the error in the ouput layer\n",
    "        error = (activations[-1] - y) * self.sigmoid_prime(zs[-1])\n",
    "        \n",
    "        # according to the second equation rate of change of the cost wrt \n",
    "        # the biases in any layer is equal to the error in that layer\n",
    "        grad_biases[-1] = error\n",
    "        \n",
    "        # use the fourth equation to calculate the rate of change of\n",
    "        # the cost wrt the weights in the last layer\n",
    "        grad_weights[-1] = np.dot(error, activations[-2].T)\n",
    "        \n",
    "        for i in range(2, self.num_layers):\n",
    "            # calculate error for subsequent layers using equation two\n",
    "            error = np.dot(self.weights[-i+1].T, error) * self.sigmoid_prime(zs[-i]) \n",
    "            grad_biases[-i] = error\n",
    "            grad_weights[-i] = np.dot(error, activations[-i-1].T)\n",
    "            \n",
    "        return grad_biases, grad_weights\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        predictions = [ (np.argmax(self.forward_pass(x)), y)\n",
    "                       for x, y in test_data]\n",
    "        return sum(int(x == y) for x, y in predictions)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Neural network is tested on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NN([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 8278 / 10000\n",
      "Epoch 1: 9248 / 10000\n",
      "Epoch 2: 9263 / 10000\n",
      "Epoch 3: 9340 / 10000\n",
      "Epoch 4: 9350 / 10000\n",
      "Epoch 5: 9401 / 10000\n",
      "Epoch 6: 9424 / 10000\n",
      "Epoch 7: 9416 / 10000\n",
      "Epoch 8: 9426 / 10000\n",
      "Epoch 9: 9468 / 10000\n",
      "Epoch 10: 9440 / 10000\n",
      "Epoch 11: 9478 / 10000\n",
      "Epoch 12: 9471 / 10000\n",
      "Epoch 13: 9461 / 10000\n",
      "Epoch 14: 9462 / 10000\n",
      "Epoch 15: 9469 / 10000\n",
      "Epoch 16: 9495 / 10000\n",
      "Epoch 17: 9459 / 10000\n",
      "Epoch 18: 9491 / 10000\n",
      "Epoch 19: 9489 / 10000\n",
      "Epoch 20: 9473 / 10000\n",
      "Epoch 21: 9466 / 10000\n",
      "Epoch 22: 9477 / 10000\n",
      "Epoch 23: 9485 / 10000\n",
      "Epoch 24: 9503 / 10000\n",
      "Epoch 25: 9506 / 10000\n",
      "Epoch 26: 9494 / 10000\n",
      "Epoch 27: 9495 / 10000\n",
      "Epoch 28: 9504 / 10000\n",
      "Epoch 29: 9471 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(train, epochs=30, bs=10, lr=3.0, test_data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
